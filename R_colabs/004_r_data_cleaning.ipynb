{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to chemistRy\n",
        "\n",
        "Welcome to the *chemistRy*, a product of the JMU Department of Chemistry and Biochemistry. This is the fourth lesson in the series which  aims to teach you about data visualization, data tidying, statistics, and a bit of R coding. If you don't know how to code, don't worry! These lessons assume no prior knowledge of code or R.\n",
        "\n",
        "A few things to start:\n",
        "\n",
        "1.   These lessons only work in Google Chrome\n",
        "2.   If you want to save your progress, go to File> Save a Copy in Drive; then locate a spot in your Drive folder\n",
        "3.   This is a special Google Colab for R, regular Colab runs in Python. If you want to make your own R notebook, follow this [link](https://colab.research.google.com/drive/16u6W4rl-AHX_yXlQx7HcgjAuNimfqwj_).\n",
        "\n",
        "If you have questions, feel free to contact Dr. Chris Berndsen in the JMU Chemistry Department.\n"
      ],
      "metadata": {
        "id": "yNGCBz7Y7gRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjSXlRmb8Bwp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run this block always\n",
        "#@markdown This code will install the necessary packages\n",
        "#install.packages(\"modeldata\", quietly = TRUE)\n",
        "install.packages(\"R.utils\", quietly = TRUE)\n",
        "\n",
        "## load packages\n",
        "library(tidyverse)\n",
        "#library(modeldata)\n",
        "library(R.utils)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this block to download the data\n",
        "#@markdown This code will import the data and process it into the data frame format. The CSV files are available to download in the file folder on the left side if you want to work on this project offline.\n",
        "download.file(\"https://raw.githubusercontent.com/CEBerndsen/r_colab_data/main/guinier.csv\", \"guinier.csv\")\n",
        "download.file(\"https://raw.githubusercontent.com/CEBerndsen/r_colab_data/main/porod.csv\", \"porod.csv\")\n",
        "g <- read.csv(\"guinier.csv\")\n",
        "p <- read.csv(\"porod.csv\")\n",
        "\n",
        "gdf <- as_tibble(g)\n",
        "pdf <- as_tibble(p)"
      ],
      "metadata": {
        "id": "HRwf6zc2vTqm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning\n",
        "Data comes in many shapes, sizes, and levels of complexity. Thus far we have worked with mostly tidy data, meaning it is formatted nicely for R and requires no changes to visualize. However, this is rarely the case with data we collect. We may need to organize data, remove columns or rows that are unnecessary for the analysis, or do calculations to get values with the right units. For small data sets, this is straightfoward to do manually in a spreadsheet program. However for complex or large data sets, this task becomes much more difficult to do efficiently by hand.\n",
        "\n",
        "Take a look at the data set below:"
      ],
      "metadata": {
        "id": "lFbSgFoL4ki5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim(gdf)\n",
        "head(gdf)\n"
      ],
      "metadata": {
        "id": "BgkT9nsjxqZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set has 7200 rows of observations for 11 different values. In addition, but not shown, there is a second data set that needs to be paired with this one row by row. This is BIG DATA!\n",
        "\n",
        "Here are the tasks we need to accomplish:\n",
        "\n",
        "\n",
        "*   Combine the two data frames `gdf` and `pdf` and match up the datasets\n",
        "*   Remove the path from the filename column\n",
        "*   Remove the duplicate columns and some unnecessary data columns\n",
        "*   Calculate the estimated $D_{max}$ value\n",
        "\n",
        "Once we have the cleaned data set, we can summarize and visualize the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "CDtgovRX58qJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joining data frames\n",
        "When you want to combine observations on the same data set from two sources, it is easy to think that you could just copy and paste the two spreadsheets and be done. But that act assumes the data are in the same order so that you can match the rows. This is not always the case especially with complex data sets. In R, we can use the `join` function to match and combine the two data frames together.\n",
        "\n",
        "This [page](https://tavareshugo.github.io/r-intro-tidyverse-gapminder/08-joins/index.html) shows a nice visual of the different types of joins that are possible. We will be performing an `inner_join`, which means only rows that exist in both dataframes will be in the combined data frame.\n",
        "\n",
        "To join data frames, you need to have a common column to match in both data sets. In our case  `filename`, `frame_number`, `well_num`, or `buffer` are common between the data sets. The `filename` is the ***most specific*** and the easiest choice. We could use a combination of `frame_number`, `well_num`, and `buffer` and that is probably the tidier choice.\n",
        "\n",
        "`inner_join` takes 3 parameters: dataframe 1, dataframe 2, and `by=` which is the column name.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQPVFmjo7ejK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cdf is the new dataframe reference\n",
        "# gdf and pdf are the two dataframes we are joining by filename\n",
        "cdf <- inner_join(gdf, pdf, by = \"filename\")\n",
        "\n",
        "# see the first 6 rows of the new data frame\n",
        "head(cdf)\n",
        "dim(cdf)"
      ],
      "metadata": {
        "id": "wtqMcvJ69eBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the dimensions of the dataframe listed below the table. The new dataframe has the same number of observations just for more variables. You may see some of the columns have a .x or .y in the name, that is because they were duplicated in the join. While filename was also in common it was the name we matched, so it does not get a .x or .y suffix. We can eliminate some of these duplicates by joining another way."
      ],
      "metadata": {
        "id": "tylF9ned_5Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use well_num, buffer, and frame_number to match up the data\n",
        "# HINT: by= typically takes only a single value, how do we combine multiple values in a single name?\n",
        "# refer to 001_r_intro if you forgot\n",
        "cdf2 <- inner_join(gdf, pdf, by = )\n",
        "\n",
        "head(cdf2)\n",
        "dim(cdf2)"
      ],
      "metadata": {
        "id": "z7-RMKZJ93XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you were successful in making `cdf2` then you should see a table that looks similar to the first join, but the dimensions are different. There are fewer columns but the same number of rows. Also, frame_number lacks the .x suffix, because we used that column along with couple more to join the data. Only filename has the .x.\n",
        "\n",
        "What if you modify the code to include `filename` along with `frame_number`, `well_num`, or `buffer`? Try this!\n"
      ],
      "metadata": {
        "id": "OMpepC-S_0vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the data\n",
        "We now have joined the datasets and we did some cleaning inadvertantly when we used multiple factors to join the dataframes as this removed duplicate columns. But there is still more to do! The new dataframe has some extra columns and the observations in the filename column include the path. Now we will use the `separate` command to break about the filename column and the `select` commmand to remove extra columns. Finally, we will remove `NA` values using the filter command. We can do this step-wise and in one step and will learn both.\n",
        "\n",
        "## Separating data in columns\n",
        "The `separate` command splits observations in a column by some character or value. In our data, the filenames have the path which includes `\\` between the directories, so we will split using that value."
      ],
      "metadata": {
        "id": "bX7b2V_M9cef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to provide dataframe name, column name, and the separator, and the new column names.\n",
        "# add the dataframe name and column name removing the _____\n",
        "# also name the new columns A, B, C, filename, remember you need to combine multiple names into a single assignment\n",
        "cdf3 <- separate(cdf2, _______, sep = \"\\\\\\\\\",  into = _______))\n",
        "head(cdf3)"
      ],
      "metadata": {
        "id": "MX-s9jo5-ZMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success! We split the column, now we need to remove columns A, B, and C along with a few others.\n",
        "\n",
        "**Note:** You might have noticed that the `sep` command was filled in already, that's because `\\` is treated unusually in R and needs to be \"escaped\". Typically we escape a special character with `\\\\` (ex. `\\\\.` or `\\\\?` but since we are literally wanting a `\\` we need to escape it twice, thus the `\\\\\\\\`.\n",
        "\n",
        "## Selecting columns\n",
        "The command for selecting columns is `select` and like separate it requires a dataframe and the columns to keep. These can be indicated by names or by column number if known.\n",
        "\n"
      ],
      "metadata": {
        "id": "pemWyKlC_Bbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to keep filename, frame_number, well_num, buffer, rg_med, mass_vp, mass_vc\n",
        "# put them in the space indicated\n",
        "# no need for quotes around column names in this case.\n",
        "cdf4 <- select(cdf3, ______)\n",
        "\n",
        "head(cdf4)"
      ],
      "metadata": {
        "id": "wGYsqvMKGXuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# by column number starting from left at 1\n",
        "cdf5 <- select(cdf3, 5:9, 14, 21)\n",
        "\n",
        "head(cdf5)"
      ],
      "metadata": {
        "id": "_6NTWYcKHBPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using column numbers is easier than typing all that text, so use that if at all possible.\n",
        "\n",
        "# Removing rows\n",
        "\n",
        "Finally, we will remove the rows that contain `NA` values using the `filter` command. These values will affect the calculations and summarizing the data that comes next. However, sometimes they can be informative so filter NA values selectively. You can also filter based on specific values like names (ex. buffer name) or certain limits (ex. rg_med > 10).\n",
        "\n",
        "Let's remove the `NA` values and make sure our rg_med values are all positive. `Filter` requires a dataframe and some sort of TRUE/FALSE statement like `buffer == \"Buffer1\"`. In this example if the value in buffer is Buffer1, then the statement is evaluated as TRUE and the row is kept, while if it is FALSE, then the entire row is discarded."
      ],
      "metadata": {
        "id": "gHlCJX6sHcxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the true\\false statement saying rg_med is greater than 0\n",
        "cdf6 <- filter(cdf5, _______)\n",
        "\n",
        "# remove the na values\n",
        "# R has an is.na(), but we need the opposite of that so we add an ! in front of the command\n",
        "cdf7 <- filter(cdf6, !is.na(rg_med))\n",
        "\n",
        "dim(cdf7)"
      ],
      "metadata": {
        "id": "_dSwqzDCIYoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing calculations and adding columns\n",
        "Sometimes we want to do additional processing like multiplying numbers together or correcting factors. We can do this using the `mutate` function. In this step, we write an equation to show how we want R to do the math. Here we will estimate the Dmax value by multiplying the median Rg by 3."
      ],
      "metadata": {
        "id": "DiCKvZMdNTNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the equation to multiply rg_med by 3\n",
        "cdf8 <- mutate(cdf7, d_est = _____)\n",
        "\n",
        "head(cdf8)"
      ],
      "metadata": {
        "id": "EgwdCuy0N9I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did the cleaning in several different steps, but we could do it all in one set of code if we used \"pipes\". The symbol for a pipe is `%>%` or `|>` and allows you to feed the output of one function into another. See the piped version of our previous work below."
      ],
      "metadata": {
        "id": "9M1jApGsLVJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdf9 <- cdf2 %>%\n",
        "  separate(., filename, sep = \"\\\\\\\\\",  into = c(\"A\", \"B\", \"C\", \"filename\")) %>%\n",
        "  select(., 5:9, 14, 21) %>%\n",
        "  filter(rg_med > 0) %>%\n",
        "  mutate(., d_est = rg_med*3)\n",
        "\n",
        "head(cdf9)\n",
        "dim(cdf9)"
      ],
      "metadata": {
        "id": "r_N_AzMuLoIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "See how much easier that was! We did not have to keep track of a bunch of intermediate dataframe steps and we got the same answer.\n",
        "\n",
        "Now that we have a clean data set, we can move onto summarizing the data both visually and statistically.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Summarizing data\n",
        "\n",
        "Big data sets are fun to visualize, but sometimes we need the numbers and statistics to help form conclusions. The current data set is >7000 rows long and composed of 30-50 measurements (`frame_number`) of several unique combinations of samples (`well number`) and buffer (`buffer`). To try to summarize this manually would be difficult and tedious. Fortunately, R offer a few methods to summarize the data. The first is a simple summary of the data frame which works well for homogenous data.\n"
      ],
      "metadata": {
        "id": "tJzIAUksP70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of the cdf9\n",
        "summary(cdf9)"
      ],
      "metadata": {
        "id": "xw1v-0-xw0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command produced the basic statistics for the observations in each column but these statistics are meaningless because we have observations from a lot of different samples. To be useful, we want the statistics for all fifty frame numbers for each well number and buffer combination. To do this we are going to use a pipe along with two new functions `group_by` and `summarize` to tell R how to divide the data and then calculate the statistics. Like Excel, R has built in functions for `mean` and standard deviation (`sd`) so we can use those inside the summarize function.\n"
      ],
      "metadata": {
        "id": "9mAiIwqTxXc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df <- cdf9 %>%\n",
        "  # fill in the column names that we want to group by with a comma between the names. There are 2 columns names needed here\n",
        "  group_by(.,________) %>%\n",
        "  summarize(mean_rg = mean(rg_med),\n",
        "            sd_rg = sd(rg_med),\n",
        "            # fill in the equations to calculate the mean and sd of Vp, add comma and a line break each time\n",
        "            )\n",
        "\n",
        "head(summary_df)"
      ],
      "metadata": {
        "id": "RGHXCrl_O6tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summarized data file is greatly condensed and *hopefully* shows the mean and standard deviation for the desired value for each well and buffer combination. The group_by and summarize combination is an incredibly useful tool in data analysis.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Now you try with two new sets of data. Like the examples above, these are analysis of small angle X-ray scattering data and the resulting stats. However, they are from two sources, one an internal analysis and the other from the [SASBDB](https://www.sasbdb.org/), the international repository for SAXS data. We want to compare the internal analysis to the deposited numbers to determine the validity of the new analysis. This will require a few cleaning steps, joining, the data, and then calculation of the differences between experimental and reference values. The specific tasks are listed below:\n",
        "\n",
        "\n",
        "1.   Separate the filename column in exp removing the filepath and .dat leaving only the id value (ex. SASDA26)\n",
        "\n",
        "2.   Convert the Rg value in db from nanometers to Angstroms (rg_A)\n",
        "\n",
        "3.   Join the exp and db data frames by the id value\n",
        "\n",
        "4.   Reduce the combined dataframe by keeping only rg_ts, rg_A, mass_vp, mass, N (from exp) and N (from db)\n",
        "\n",
        "5.   Determine the differences between rg_ts and rg_A, mass_vp and mass, and N and N.\n",
        "\n",
        "6.   Calculate the mean, median, sd, and IQR for each difference (does not require the group_by %>% summary path)\n",
        "\n",
        "7.   Plot distribution of differences in ggplot2 for each comparison and show the stats on the plot using an `annotate` layer.\n",
        "\n",
        "##Good luck!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FlW0ZqERQASJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load the data\n",
        "download.file(\"https://raw.githubusercontent.com/CEBerndsen/r_colab_data/main/SASBDB_analysis.csv\", \"exp.csv\")\n",
        "download.file(\"https://raw.githubusercontent.com/CEBerndsen/r_colab_data/main/SASBDB_protein_stats.csv\", \"db.csv\")\n",
        "e <- read.csv(\"exp.csv\")\n",
        "p <- read.csv(\"db.csv\")\n",
        "\n",
        "exp <- as_tibble(e)\n",
        "db <- as_tibble(p)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L3KODOVjW18u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the layout of the experimental analysis\n",
        "head(exp)"
      ],
      "metadata": {
        "id": "6kXzXWviXPlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the layout of the database\n",
        "head(db)"
      ],
      "metadata": {
        "id": "-QkBnljcXVX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn!"
      ],
      "metadata": {
        "id": "BJTbyt53Xcy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}